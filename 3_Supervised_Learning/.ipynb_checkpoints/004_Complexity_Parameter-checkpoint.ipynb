{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 복잡도 파라미터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복잡도 파라미터 ; 복잡도에 영향을 주는 하이퍼파라미터\n",
    "- 이 값에 따라 과적합 정도가 결정된다. \n",
    "- 매우 신중한 튜닝이 필요하다. \n",
    "\n",
    "\n",
    "- 휴리스틱하게 모든 모델 : 학습할때마다 결과가 조금씩 다를 수있는 모든 모델을 의미함. Solution이 정확히 있는게 아니면 학습마다 달라질 수 있음. \n",
    "    - `max_iter`가 있으면, 휴리스틱하게 학습되고 있는 모델이라고 봐도 문제가 없음. \n",
    "    - max_iter가 작으면, convergence warning이 뜰 수 있음. 즉, 충분히 수렴 하지 않은 것. \n",
    "    - 그러나 여기서 중요한 것은 덜 학습된 모델이 더 학습된 모델보다 더 성능이 안좋다고는 확실히 말하기 어려움. \n",
    "    - 오히려 더 general한 모델일 가능성도 있음. 과적합을 막아 준 것 일수도 있음. \n",
    "    - 그래서, 일부 모델에서는 `max_iter` 값을 일부러 작게 잡아서 과적합을 회피하기도 한다.  \n",
    "    \n",
    "- 정규화 회귀 모델에서의 `alpha` : 복잡도와 반비례 관계 \n",
    "    - 패널티가 많이 부과될수록 모델이 단순해 지고 계수의 절댓값이 더 작아진다. \n",
    "\n",
    "- 의사결정나무 \n",
    "    - `max_depth` : 깊게 가도 될수록 나무는 더 복잡해 진다. \n",
    "    - `min_samples_leaf` : 이것을 크게 놓을 수록 덜 복잡해진다. \n",
    "    \n",
    "- SVM\n",
    "    - C, gamma, degree : 약한 정비례 - 직접적으로 복잡도에 영향을 줄 수도 있고, 안줄 수도 있다. 굳이 비교하자면, 상관계수 약 0.2정도라고 생각. \n",
    "\n",
    "\n",
    "    \n",
    "![3_7.png](../materials/3_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀 \n",
    "- 신경망 \n",
    "    - 노드가 많아지고 복잡해질수록 과적합\n",
    "- SVR : 허용 오차 범위 epsilon이 클 수록, 복잡도가 줄어든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_7.png](../materials/3_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_7.png](../materials/3_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed도 일종의 하이퍼파라미터가 될 수 있음. 좋은 seed도 따로 있을 수 있음. 이 또한 테스트 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3_7.png](../materials/3_10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(r\"/Users/sanghyuk/Documents/preprocessing_python/lecture_source/Part 3. 지도학습 주요모델 및 개념/데이터/\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Sonar_Mines_Rocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징과 라벨 분리\n",
    "X = df.drop('Y', axis = 1)\n",
    "Y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 평가 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 60)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.shape # 샘플 156개, 특징 60개 => 단순한 모델 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    86\n",
       "R    70\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y.replace({\"M\":-1, \"R\":1}, inplace = True)\n",
    "Test_Y.replace({\"M\":-1, \"R\":1}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53    1\n",
       "84    1\n",
       "66    1\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. 복잡도 파라미터가 한 개이면서, 단순하고, 우연성이 어느정도 있는 모델 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model_test(C):\n",
    "    model = LR(C = C, max_iter = 100000, random_state = 10).fit(Train_X, Train_Y) # 가벼운 모델이므로 max_iter를 크게 잡음\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1:\t0.6829268292682926\n",
      "C = 1:\t0.92\n",
      "C = 5:\t0.8979591836734693\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.1:\\t{}\".format(LR_model_test(C = 0.1)))\n",
    "print(\"C = 1:\\t{}\".format(LR_model_test(C = 1))) \n",
    "print(\"C = 5:\\t{}\".format(LR_model_test(C = 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝 범위: 0.1 < C < 5\n",
    "# 아직 확정짓기에는 범위가 넓다 => 더 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.5:\t0.8750000000000001\n",
      "C = 2:\t0.92\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.5:\\t{}\".format(LR_model_test(C = 0.5)))\n",
    "print(\"C = 2:\\t{}\".format(LR_model_test(C = 2)))\n",
    "\n",
    "# 튜닝 범위: 0.1 < C < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5224489795918368, 'max_iter': 100000, 'random_state': 10} 0.92\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "LR_parameter_grid = ParameterGrid({\"C\":np.linspace(0.1, 1, 50),\n",
    "                                  \"max_iter\":[100000],\n",
    "                                  \"random_state\":[10]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in LR_parameter_grid:\n",
    "    model = LR(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. 복잡도 파라미터가 두 개이면서, 단순하고, 우연성이 거의 없는 모델 (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTC_model_test(max_depth, min_samples_leaf):\n",
    "    model = DTC(max_depth = max_depth, min_samples_leaf = min_samples_leaf).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-1:0.8070175438596491\n",
      "3-2:0.75\n",
      "3-3:0.75\n",
      "6-1:0.7586206896551724\n",
      "6-2:0.7924528301886792\n",
      "6-3:0.75\n",
      "9-1:0.7407407407407407\n",
      "9-2:0.7719298245614035\n",
      "9-3:0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [3, 6, 9]:\n",
    "    for min_samples_leaf in [1, 2, 3]:\n",
    "        score = DTC_model_test(max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
    "        print(\"{}-{}:{}\".format(max_depth, min_samples_leaf, score))\n",
    "\n",
    "# max depth가 크고 (복잡도 증가) min_samples_leaf가 큰 경우 (복잡도 감소) 좋은 성능이 나옴을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 7, 'min_samples_leaf': 4} 0.8275862068965517\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "DTC_parameter_grid = ParameterGrid({\"max_depth\": np.arange(6, 15),\n",
    "                                  \"min_samples_leaf\": np.arange(2, 5)})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in DTC_parameter_grid:\n",
    "    model = DTC(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3. 복잡도 파라미터가 하나이면서, 우연성이 있는 모델 (신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model_test(hidden_layer_sizes):\n",
    "    model = MLP(hidden_layer_sizes = hidden_layer_sizes, random_state = 12).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,) 0.6835443037974684\n",
      "(10,) 0.7916666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) 0.25\n",
      "(5, 5) 0.0\n",
      "(10, 10) 0.851063829787234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# (5, ) 은닉층 1개이면서 노드 5개\n",
    "for hidden_layer_sizes in [(5, ), (10, ), (3, 3), (5, 5), (10, 10)]:    \n",
    "    score = MLP_model_test(hidden_layer_sizes = hidden_layer_sizes)\n",
    "    print(hidden_layer_sizes, score)\n",
    "\n",
    "# max_iter warning 발생 \n",
    "# (5, 5)에서는 f1-score가 0이 나옴 => 초기값 영향으로 보여짐 (근거: 더 단순한 모델과 복잡한 모델에서는 성능이 나왔으므로)\n",
    "# (10, )와 (10, 10)에서 best_score가 나옴 => 더 복잡한 모델이 필요할지 판단이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5, 5) 어떻게 0이 나오지? 초깃값 영향이 클 확률이 높음. 신경망은 이렇게 뭔가 겉잡을 수가 없음. 테스트 많이 해봐야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/sanghyuk/anaconda/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (10, 10, 10), 'max_iter': 2000, 'random_state': 41} 0.923076923076923\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "MLP_parameter_grid = ParameterGrid({\"random_state\": [41, 102, 15],\n",
    "                                  \"hidden_layer_sizes\": [(5, 5), (10, 10), (5, 5, 5), (10, 10, 10)],\n",
    "                                   \"max_iter\":[200, 2000, 20000]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in MLP_parameter_grid:\n",
    "    model = MLP(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
