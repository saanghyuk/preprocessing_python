{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정 및 데이터 불러오기\n",
    "import os\n",
    "os.chdir(r\"D:\\강의 자료\\2008_온라인 강의 머신러닝 성능 향상을 위한 데이터 탐색과 전처리 심화\\데이터\\실습 데이터\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"Sonar_Mines_Rocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징과 라벨 분리\n",
    "X = df.drop('Y', axis = 1)\n",
    "Y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 평가 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.shape # 샘플 156개, 특징 60개 => 단순한 모델 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    83\n",
       "R    73\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y.replace({\"M\":-1, \"R\":1}, inplace = True)\n",
    "Test_Y.replace({\"M\":-1, \"R\":1}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. 복잡도 파라미터가 한 개이면서, 단순하고, 우연성이 어느정도 있는 모델 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model_test(C):\n",
    "    model = LR(C = C, max_iter = 100000, random_state = 10).fit(Train_X, Train_Y) # 가벼운 모델이므로 max_iter를 크게 잡음\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1:\t0.5714285714285715\n",
      "C = 1:\t0.6530612244897959\n",
      "C = 5:\t0.6382978723404256\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.1:\\t{}\".format(LR_model_test(C = 0.1)))\n",
    "print(\"C = 1:\\t{}\".format(LR_model_test(C = 1))) \n",
    "print(\"C = 5:\\t{}\".format(LR_model_test(C = 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝 범위: 0.1 < C < 5\n",
    "# 아직 확정짓기에는 범위가 넓다 => 더 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.5:\t0.68\n",
      "C = 2:\t0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"C = 0.5:\\t{}\".format(LR_model_test(C = 0.5)))\n",
    "print(\"C = 2:\\t{}\".format(LR_model_test(C = 2)))\n",
    "\n",
    "# 튜닝 범위: 0.1 < C < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.3387755102040817, 'max_iter': 100000, 'random_state': 10} 0.6938775510204083\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "LR_parameter_grid = ParameterGrid({\"C\":np.linspace(0.1, 1, 50),\n",
    "                                  \"max_iter\":[100000],\n",
    "                                  \"random_state\":[10]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in LR_parameter_grid:\n",
    "    model = LR(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2. 복잡도 파라미터가 두 개이면서, 단순하고, 우연성이 거의 없는 모델 (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTC_model_test(max_depth, min_samples_leaf):\n",
    "    model = DTC(max_depth = max_depth, min_samples_leaf = min_samples_leaf).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-1:0.7307692307692307\n",
      "3-2:0.7450980392156864\n",
      "3-3:0.7450980392156864\n",
      "6-1:0.7307692307692307\n",
      "6-2:0.7199999999999999\n",
      "6-3:0.7692307692307692\n",
      "9-1:0.7450980392156864\n",
      "9-2:0.6808510638297872\n",
      "9-3:0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [3, 6, 9]:\n",
    "    for min_samples_leaf in [1, 2, 3]:\n",
    "        score = DTC_model_test(max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
    "        print(\"{}-{}:{}\".format(max_depth, min_samples_leaf, score))\n",
    "\n",
    "# max depth가 크고 (복잡도 증가) min_samples_leaf가 큰 경우 (복잡도 감소) 좋은 성능이 나옴을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'min_samples_leaf': 3} 0.7692307692307692\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "DTC_parameter_grid = ParameterGrid({\"max_depth\": np.arange(6, 15),\n",
    "                                  \"min_samples_leaf\": np.arange(2, 5)})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in DTC_parameter_grid:\n",
    "    model = DTC(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3. 복잡도 파라미터가 하나이면서, 우연성이 있는 모델 (신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model_test(hidden_layer_sizes):\n",
    "    model = MLP(hidden_layer_sizes = hidden_layer_sizes, random_state = 12).fit(Train_X, Train_Y) \n",
    "    pred_Y = model.predict(Test_X)\n",
    "    return f1_score(Test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,) 0.631578947368421\n",
      "(10,) 0.6956521739130435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) 0.30303030303030304\n",
      "(5, 5) 0.0\n",
      "(10, 10) 0.6382978723404256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for hidden_layer_sizes in [(5, ), (10, ), (3, 3), (5, 5), (10, 10)]:    \n",
    "    score = MLP_model_test(hidden_layer_sizes = hidden_layer_sizes)\n",
    "    print(hidden_layer_sizes, score)\n",
    "\n",
    "# max_iter warning 발생 \n",
    "# (5, 5)에서는 f1-score가 0이 나옴 => 초기값 영향으로 보여짐 (근거: 더 단순한 모델과 복잡한 모델에서는 성능이 나왔으므로)\n",
    "# (10, )와 (10, 10)에서 best_score가 나옴 => 더 복잡한 모델이 필요할지 판단이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\GilseungAhn\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (10, 10, 10), 'max_iter': 2000, 'random_state': 41} 0.8\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 그리드 설정\n",
    "MLP_parameter_grid = ParameterGrid({\"random_state\": [41, 102, 15],\n",
    "                                  \"hidden_layer_sizes\": [(5, 5), (10, 10), (5, 5, 5), (10, 10, 10)],\n",
    "                                   \"max_iter\":[200, 2000, 20000]})\n",
    "\n",
    "# 파라미터 튜닝 수행 \n",
    "best_score = -1\n",
    "for parameter in MLP_parameter_grid:\n",
    "    model = MLP(**parameter).fit(Train_X, Train_Y)\n",
    "    pred_Y = model.predict(Test_X)\n",
    "    score = f1_score(Test_Y, pred_Y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_parameter = parameter\n",
    "\n",
    "print(best_parameter, best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
